{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d3BMQvHXYIMy"
      },
      "outputs": [],
      "source": [
        "!pip -q install torchcodec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_sKuVJDp7qT",
        "outputId": "46f8ae56-00ef-4dae-c042-98c081119dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None,)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yruvSzHEDLY",
        "outputId": "ac022a90-5d5b-4595-cfee-8563f32aeab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   45G  191G  20% /\n",
            "2.8G\t/content/drive/MyDrive/openmic-2018-2\n",
            "673M\t/content/drive/MyDrive/deep_learning/sentetik-dataset\n"
          ]
        }
      ],
      "source": [
        "!df -h /content\n",
        "!du -sh \"/content/drive/MyDrive/openmic-2018-2\"\n",
        "!du -sh \"/content/drive/MyDrive/deep_learning/sentetik-dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JAtI5AvMEJmb"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/openmic-2018-2\n",
        "!mkdir -p /content/sentetik-dataset\n",
        "\n",
        "# Copy OpenMIC audio folder + metadata files\n",
        "!cp -r \"/content/drive/MyDrive/openmic-2018-2/audio\" \"/content/openmic-2018-2/\"\n",
        "!cp \"/content/drive/MyDrive/openmic-2018-2/openmic-2018-aggregated-labels.csv\" \"/content/openmic-2018-2/\" || true\n",
        "!cp \"/content/drive/MyDrive/openmic-2018-2/openmic-2018-metadata.csv\" \"/content/openmic-2018-2/\" || true\n",
        "\n",
        "# Copy Synthetic audio + labels\n",
        "!cp -r \"/content/drive/MyDrive/deep_learning/sentetik-dataset/audio\" \"/content/sentetik-dataset/\"\n",
        "!cp \"/content/drive/MyDrive/deep_learning/sentetik-dataset/labels.csv\" \"/content/sentetik-dataset/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf5kE-2XLbsv",
        "outputId": "4552909a-db9c-409c-8f29-61a14b394727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing paths in sample(500): 0\n",
            "source\n",
            "openmic      20000\n",
            "synthetic     2200\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = Path(\"/content/drive/MyDrive/deep_learning/combined_openmic_and_synth.csv\")\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "\n",
        "# Patch to local /content paths (since you copied audio locally)\n",
        "df[\"path\"] = df[\"path\"].str.replace(\n",
        "    \"/content/drive/MyDrive/openmic-2018-2\",\n",
        "    \"/content/openmic-2018-2\",\n",
        "    regex=False\n",
        ").str.replace(\n",
        "    \"/content/drive/MyDrive/deep_learning/sentetik-dataset\",\n",
        "    \"/content/sentetik-dataset\",\n",
        "    regex=False\n",
        ")\n",
        "\n",
        "# Sanity: verify some paths exist\n",
        "missing = sum(not Path(p).exists() for p in df[\"path\"].sample(500, random_state=0))\n",
        "print(\"Missing paths in sample(500):\", missing)\n",
        "print(df[\"source\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5OHUEw2LhXW",
        "outputId": "fa6af452-03d0-4ded-ceef-d8aab824ffc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata candidates: ['openmic-2018-metadata.csv', 'openmic-2018-metadata.csv']\n",
            "Using metadata: openmic-2018-metadata.csv\n",
            "No split column found; using random split for OpenMIC.\n",
            "split\n",
            "train    18200\n",
            "test      2000\n",
            "valid     2000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "OPENMIC_TAGS = [\n",
        "    \"accordion\",\"banjo\",\"bass\",\"cello\",\"clarinet\",\"cymbals\",\"drums\",\"flute\",\"guitar\",\n",
        "    \"mallet_percussion\",\"mandolin\",\"organ\",\"piano\",\"saxophone\",\"synthesizer\",\n",
        "    \"trombone\",\"trumpet\",\"ukulele\",\"violin\",\"voice\"\n",
        "]\n",
        "Y_COLS = [f\"y_{t}\" for t in OPENMIC_TAGS]\n",
        "M_COLS = [f\"m_{t}\" for t in OPENMIC_TAGS]\n",
        "\n",
        "OPENMIC_DIR = Path(\"/content/openmic-2018-2\")\n",
        "meta_candidates = list(OPENMIC_DIR.glob(\"*metadata*.csv\")) + list(OPENMIC_DIR.glob(\"*meta*.csv\"))\n",
        "print(\"Metadata candidates:\", [p.name for p in meta_candidates])\n",
        "\n",
        "# Default: synthetic always train\n",
        "df[\"split\"] = \"train\"\n",
        "\n",
        "# Try official split if metadata exists\n",
        "if meta_candidates:\n",
        "    meta_path = meta_candidates[0]\n",
        "    meta = pd.read_csv(meta_path, low_memory=False)\n",
        "    print(\"Using metadata:\", meta_path.name)\n",
        "\n",
        "    split_col = None\n",
        "    for c in [\"split\", \"subset\", \"partition\"]:\n",
        "        if c in meta.columns:\n",
        "            split_col = c\n",
        "            break\n",
        "\n",
        "    if split_col is not None:\n",
        "        if \"sample_key\" not in meta.columns:\n",
        "            if \"filename\" in meta.columns:\n",
        "                meta[\"sample_key\"] = meta[\"filename\"].astype(str).str.replace(\".ogg\",\"\", regex=False)\n",
        "            else:\n",
        "                raise RuntimeError(\"Metadata has no sample_key/filename to join.\")\n",
        "\n",
        "        meta_small = meta[[\"sample_key\", split_col]].copy()\n",
        "        meta_small[\"filename\"] = meta_small[\"sample_key\"].astype(str) + \".ogg\"\n",
        "\n",
        "        open_mask = df[\"source\"].astype(str).eq(\"openmic\")\n",
        "        joined = df.loc[open_mask, [\"filename\"]].merge(meta_small[[\"filename\", split_col]], on=\"filename\", how=\"left\")\n",
        "\n",
        "        s = joined[split_col].astype(str).str.lower()\n",
        "        s = s.replace({\"validation\":\"valid\",\"val\":\"valid\"})\n",
        "        df.loc[open_mask, \"split\"] = s.values\n",
        "    else:\n",
        "        print(\"No split column found; using random split for OpenMIC.\")\n",
        "        meta_candidates = []\n",
        "else:\n",
        "    print(\"No metadata found; using random split for OpenMIC.\")\n",
        "\n",
        "# Random split if no official split\n",
        "if not meta_candidates:\n",
        "    rng = np.random.default_rng(42)\n",
        "    open_idx = df.index[df[\"source\"].astype(str).eq(\"openmic\")].to_numpy()\n",
        "    rng.shuffle(open_idx)\n",
        "\n",
        "    n = len(open_idx)\n",
        "    n_train = int(0.8*n)\n",
        "    n_valid = int(0.1*n)\n",
        "\n",
        "    df.loc[open_idx[n_train:n_train+n_valid], \"split\"] = \"valid\"\n",
        "    df.loc[open_idx[n_train+n_valid:], \"split\"] = \"test\"\n",
        "\n",
        "print(df[\"split\"].value_counts(dropna=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sho-0rEVN_8N",
        "outputId": "0c0f3baa-b9c7-4bb8-99b8-06ba3debb789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18200 2000 2000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchaudio\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TARGET_SR = 16000\n",
        "DURATION_SEC = 10\n",
        "TARGET_LEN = TARGET_SR * DURATION_SEC\n",
        "\n",
        "class AudioTagDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.paths = self.df[\"path\"].astype(str).tolist()\n",
        "        self.y = torch.tensor(self.df[Y_COLS].values, dtype=torch.float32)\n",
        "        self.m = torch.tensor(self.df[M_COLS].values, dtype=torch.float32)\n",
        "        self.source = self.df[\"source\"].astype(str).tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _pad_or_crop(self, wav):\n",
        "        if wav.numel() >= TARGET_LEN:\n",
        "            return wav[:TARGET_LEN]\n",
        "        return F.pad(wav, (0, TARGET_LEN - wav.numel()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav, sr = torchaudio.load(self.paths[idx])   # [C,T]\n",
        "        wav = wav.mean(dim=0)                        # mono [T]\n",
        "        if sr != TARGET_SR:\n",
        "            wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
        "        wav = self._pad_or_crop(wav)\n",
        "        mx = wav.abs().max().clamp_min(1e-8)\n",
        "        wav = (wav / mx).to(torch.float32)\n",
        "        return wav, self.y[idx], self.m[idx], self.source[idx]\n",
        "\n",
        "train_df = df[df[\"split\"].eq(\"train\")].copy()\n",
        "valid_df = df[df[\"split\"].eq(\"valid\")].copy()\n",
        "test_df  = df[df[\"split\"].eq(\"test\")].copy()\n",
        "\n",
        "train_ds = AudioTagDataset(train_df)\n",
        "valid_ds = AudioTagDataset(valid_df)\n",
        "test_ds  = AudioTagDataset(test_df)\n",
        "\n",
        "print(len(train_ds), len(valid_ds), len(test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45DVXqG3Of86"
      },
      "source": [
        "# 4 — Log-mel transform on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P81ZRDO8ON4g"
      },
      "outputs": [],
      "source": [
        "# Recreate mel transform (same params)\n",
        "mel = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=TARGET_SR,\n",
        "    n_fft=1024,\n",
        "    hop_length=320,\n",
        "    win_length=1024,\n",
        "    n_mels=128,\n",
        "    f_min=30,\n",
        "    f_max=7600,\n",
        "    power=2.0,\n",
        ").to(DEVICE)\n",
        "\n",
        "def wav_to_logmel(wav_batch: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav_batch: [B, T] float32/float16 on GPU\n",
        "    returns:   [B, 128, frames] float32, finite\n",
        "    \"\"\"\n",
        "    # Always compute features in float32 (avoid AMP issues)\n",
        "    wav_batch = wav_batch.float()\n",
        "\n",
        "    # Disable autocast for feature extraction\n",
        "    with torch.amp.autocast(\"cuda\", enabled=False):\n",
        "        x = mel(wav_batch)                    # [B,128,F] non-negative\n",
        "        x = torch.clamp(x, min=1e-10)         # prevent log(0)\n",
        "        x = torch.log(x)                      # log-power mel (stable)\n",
        "\n",
        "        # per-sample normalization\n",
        "        mean = x.mean(dim=(1,2), keepdim=True)\n",
        "        std  = x.std(dim=(1,2), keepdim=True).clamp_min(1e-4)\n",
        "        x = (x - mean) / std\n",
        "\n",
        "        # hard sanitize (guarantee finite)\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mHJCJYkSRA1"
      },
      "source": [
        "# 5 — DataLoaders (with synthetic oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KuRkhu6oOab3"
      },
      "outputs": [],
      "source": [
        "#BATCH_SIZE = 64  # if OOM -> 32\n",
        "#\n",
        "#weights = np.where(train_df[\"source\"].astype(str).values == \"synthetic\", 4.0, 1.0).astype(np.float64)\n",
        "#sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
        "#\n",
        "#train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "#valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "#test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "BATCH_SIZE = 256  # A100 start point (if OOM -> 192 or 128)\n",
        "\n",
        "weights = np.where(train_df[\"source\"].astype(str).values == \"synthetic\", 3.0, 1.0).astype(np.float64)\n",
        "sampler = WeightedRandomSampler(weights, num_samples=len(train_ds), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FO77cWOTYCe"
      },
      "source": [
        "# 6A—B. Confirm what the checkpoint contains and how to rebuild the module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnM4mlGnQRK3",
        "outputId": "0317abc9-547b-451e-ed19-7bf8023dd7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded object type: <class 'collections.OrderedDict'>\n",
            "Top-level keys: ['0.weight', '0.bias', '3.ca.mlp.0.weight', '3.ca.mlp.0.bias', '3.ca.mlp.2.weight', '3.ca.mlp.2.bias', '3.sa.conv.weight', '3.sa.conv.bias', '4.weight', '4.bias', '7.ca.mlp.0.weight', '7.ca.mlp.0.bias', '7.ca.mlp.2.weight', '7.ca.mlp.2.bias', '7.sa.conv.weight', '7.sa.conv.bias']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_PATH = Path(\"/content/drive/MyDrive/model_weights/pretraining/pretraining_cbam_irmas_features.pth\")\n",
        "assert CKPT_PATH.exists(), f\"Checkpoint not found: {CKPT_PATH}\"\n",
        "\n",
        "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "print(\"Loaded object type:\", type(ckpt))\n",
        "\n",
        "if isinstance(ckpt, dict):\n",
        "    print(\"Top-level keys:\", list(ckpt.keys())[:40])\n",
        "    # Try common locations\n",
        "    for k in [\"model\", \"state_dict\", \"model_state_dict\", \"net\", \"weights\"]:\n",
        "        if k in ckpt and isinstance(ckpt[k], dict):\n",
        "            print(f\"Found candidate state dict under key '{k}'. Example keys:\", list(ckpt[k].keys())[:20])\n",
        "else:\n",
        "    # could be tensor/array/list = features, not weights\n",
        "    print(\"This looks like features or a tensor, not a state_dict.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW21NOMbQSUi",
        "outputId": "0b59dde3-2796-4a3b-81a0-f2cd0830401c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.weight (32, 1, 3, 3)\n",
            "0.bias (32,)\n",
            "3.ca.mlp.0.weight (4, 32)\n",
            "3.ca.mlp.0.bias (4,)\n",
            "3.ca.mlp.2.weight (32, 4)\n",
            "3.ca.mlp.2.bias (32,)\n",
            "3.sa.conv.weight (1, 2, 7, 7)\n",
            "3.sa.conv.bias (1,)\n",
            "4.weight (64, 32, 3, 3)\n",
            "4.bias (64,)\n",
            "7.ca.mlp.0.weight (8, 64)\n",
            "7.ca.mlp.0.bias (8,)\n",
            "7.ca.mlp.2.weight (64, 8)\n",
            "7.ca.mlp.2.bias (64,)\n",
            "7.sa.conv.weight (1, 2, 7, 7)\n",
            "7.sa.conv.bias (1,)\n",
            "\n",
            "conv0 weight shape: (32, 1, 3, 3)\n",
            "conv4 weight shape: (64, 32, 3, 3)\n",
            "\n",
            "CBAM3 channels: 32 hidden: 4 => reduction ~ 8\n",
            "CBAM7 channels: 64 hidden: 8 => reduction ~ 8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_PATH = Path(\"/content/drive/MyDrive/model_weights/pretraining/pretraining_cbam_irmas_features.pth\")\n",
        "state = torch.load(CKPT_PATH, map_location=\"cpu\")  # OrderedDict\n",
        "\n",
        "for k in state.keys():\n",
        "    print(k, tuple(state[k].shape))\n",
        "\n",
        "print(\"\\nconv0 weight shape:\", tuple(state[\"0.weight\"].shape))\n",
        "print(\"conv4 weight shape:\", tuple(state[\"4.weight\"].shape))\n",
        "\n",
        "# infer reduction for CBAM at index 3\n",
        "hidden3 = state[\"3.ca.mlp.0.weight\"].shape[0]\n",
        "ch3 = state[\"3.ca.mlp.0.weight\"].shape[1]\n",
        "print(\"\\nCBAM3 channels:\", ch3, \"hidden:\", hidden3, \"=> reduction ~\", ch3 // hidden3)\n",
        "\n",
        "# infer reduction for CBAM at index 7\n",
        "hidden7 = state[\"7.ca.mlp.0.weight\"].shape[0]\n",
        "ch7 = state[\"7.ca.mlp.0.weight\"].shape[1]\n",
        "print(\"CBAM7 channels:\", ch7, \"hidden:\", hidden7, \"=> reduction ~\", ch7 // hidden7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCxCrHYZTtAi"
      },
      "source": [
        "# Reinitialize model + optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ-LmPxkRlHV",
        "outputId": "47e1cb40-09f8-4e10-bfb8-a66bf6dcc4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinitialized model + reloaded pretrained features.\n"
          ]
        }
      ],
      "source": [
        "# Rebuild features and reload pretrained weights\n",
        "\n",
        "CKPT_PATH = Path(\"/content/drive/MyDrive/model_weights/pretraining/pretraining_cbam_irmas_features.pth\")\n",
        "state = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "\n",
        "# --- CBAM modules (same as before) ---\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        hidden = max(1, channels // reduction)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(channels, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, channels),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        avg = x.mean(dim=(2,3))\n",
        "        mx  = x.amax(dim=(2,3))\n",
        "        attn = torch.sigmoid(self.mlp(avg) + self.mlp(mx)).unsqueeze(-1).unsqueeze(-1)\n",
        "        return x * attn\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=True)\n",
        "    def forward(self, x):\n",
        "        avg = x.mean(dim=1, keepdim=True)\n",
        "        mx  = x.amax(dim=1, keepdim=True)\n",
        "        attn = torch.sigmoid(self.conv(torch.cat([avg, mx], dim=1)))\n",
        "        return x * attn\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels, reduction=reduction)\n",
        "        self.sa = SpatialAttention(kernel_size=7)\n",
        "    def forward(self, x):\n",
        "        return self.sa(self.ca(x))\n",
        "\n",
        "features = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding=1, bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Identity(),\n",
        "    CBAM(32, reduction=8),\n",
        "    nn.Conv2d(32, 64, 3, padding=1, bias=True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Identity(),\n",
        "    CBAM(64, reduction=8),\n",
        ")\n",
        "features.load_state_dict(state, strict=True)\n",
        "\n",
        "class CBAMTagger(nn.Module):\n",
        "    def __init__(self, features, n_tags):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, n_tags)\n",
        "        )\n",
        "    def forward(self, logmel):\n",
        "        x = logmel.unsqueeze(1)\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)\n",
        "\n",
        "model = CBAMTagger(features, n_tags=len(OPENMIC_TAGS)).to(DEVICE)\n",
        "print(\"Reinitialized model + reloaded pretrained features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdIewoFT72X"
      },
      "source": [
        "# Cell 6E - Build the full CBAM tagger (features + pooling + new 20-class head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyBrdDELTrn_",
        "outputId": "83e222f4-71e2-4c3c-e9c9-78e98df25638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready on cuda\n"
          ]
        }
      ],
      "source": [
        "class CBAMTagger(nn.Module):\n",
        "    def __init__(self, features: nn.Module, n_tags: int = 20):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 256),    # 64 comes from conv4 out_channels\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, n_tags) # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, logmel):\n",
        "        # logmel: [B, n_mels, time]\n",
        "        x = logmel.unsqueeze(1)       # [B,1,M,T]\n",
        "        x = self.features(x)          # [B,64,*,*]\n",
        "        x = self.pool(x)              # [B,64,1,1]\n",
        "        return self.head(x)           # [B,20]\n",
        "\n",
        "model = CBAMTagger(features, n_tags=len(OPENMIC_TAGS)).to(DEVICE)\n",
        "print(\"Model ready on\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awWddP9jhmIy",
        "outputId": "99099531-6d46-42c2-f993-29c0e705a9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bias init done. Bias stats: -3.729701519012451 -2.7224838733673096 -1.9965993165969849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2580264408.py:9: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  print(\"Bias init done. Bias stats:\", float(model.head[-1].bias.min()), float(model.head[-1].bias.mean()), float(model.head[-1].bias.max()))\n"
          ]
        }
      ],
      "source": [
        "# Initialize last layer bias from TRAIN priors (masked)\n",
        "p = (train_df[Y_COLS].values * train_df[M_COLS].values).sum(axis=0) / np.clip(train_df[M_COLS].values.sum(axis=0), 1, None)\n",
        "p = np.clip(p, 1e-4, 1-1e-4)\n",
        "bias = np.log(p / (1 - p))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.head[-1].bias.copy_(torch.tensor(bias, dtype=torch.float32, device=DEVICE))\n",
        "\n",
        "print(\"Bias init done. Bias stats:\", float(model.head[-1].bias.min()), float(model.head[-1].bias.mean()), float(model.head[-1].bias.max()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlz1rnmThsJE",
        "outputId": "6b22aa52-469d-4ac4-a314-e06a3102da0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_weight min/median/max: 7.363970756530762 16.71871566772461 20.0\n"
          ]
        }
      ],
      "source": [
        "Yt = torch.tensor(train_df[Y_COLS].values, dtype=torch.float32)\n",
        "Mt = torch.tensor(train_df[M_COLS].values, dtype=torch.float32)\n",
        "\n",
        "pos = (Yt * Mt).sum(dim=0)\n",
        "tot = Mt.sum(dim=0)\n",
        "neg = tot - pos\n",
        "\n",
        "pos_weight = (neg / pos.clamp_min(1.0)).clamp(max=20.0).to(DEVICE)  # cap 20 is usually stable\n",
        "print(\"pos_weight min/median/max:\",\n",
        "      float(pos_weight.min()), float(pos_weight.median()), float(pos_weight.max()))\n",
        "\n",
        "bce_w = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=pos_weight)\n",
        "\n",
        "def masked_bce_loss(logits, y, m):\n",
        "    loss = bce_w(logits, y) * m\n",
        "    denom = m.sum().clamp_min(1.0)\n",
        "    return loss.sum() / denom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-RsGC8UuFX"
      },
      "source": [
        "# Cell 7 — Masked BCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "npybgIP2T6mY"
      },
      "outputs": [],
      "source": [
        "# bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "#\n",
        "# def masked_bce_loss(logits, y, m):\n",
        "#     loss = bce(logits, y) * m\n",
        "#     denom = m.sum().clamp_min(1.0)\n",
        "#     return loss.sum() / denom\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvbW4HsYU41L"
      },
      "source": [
        "# Cell 8 — Quick forward pass sanity check (recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATIongjgJpDb",
        "outputId": "e03d370b-1e56-477a-d929-771d37323ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logmel finite? True shape: (256, 128, 501)\n",
            "logits finite? True shape: (256, 20)\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "wav, y, m, src = next(iter(train_loader))\n",
        "wav = wav.to(DEVICE)\n",
        "\n",
        "logmel = wav_to_logmel(wav)\n",
        "print(\"logmel finite?\", torch.isfinite(logmel).all().item(), \"shape:\", tuple(logmel.shape))\n",
        "logits = model(logmel)\n",
        "print(\"logits finite?\", torch.isfinite(logits).all().item(), \"shape:\", tuple(logits.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-isaKij2YV5M"
      },
      "source": [
        "# Cell 9 — Train (AMP + best checkpoint to Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pcZWRAb4ZnpK"
      },
      "outputs": [],
      "source": [
        "# model.train()\n",
        "# for i, (wav, y, m, src) in enumerate(train_loader):\n",
        "#     if i == 200:\n",
        "#         break\n",
        "#     # one step (copy from train loop)\n",
        "# print(\"Smoke test passed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o1fxgx4YUveW"
      },
      "outputs": [],
      "source": [
        "def eval_val_loss(model, loader):\n",
        "    model.eval()\n",
        "    total, denom = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for wav, y, m, src in loader:\n",
        "            wav = wav.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            m = m.to(DEVICE, non_blocking=True)\n",
        "            logits = model(wav_to_logmel(wav))\n",
        "            L = bce_w(logits, y) * m\n",
        "            total += L.sum().item()\n",
        "            denom += m.sum().item()\n",
        "    return total / max(1.0, denom)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOFC9qYjh6lD",
        "outputId": "02ed897a-b6c7-482a-de7e-a12ae7387c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 step 50/72 loss 1.9773 sec/step 1.251 ETA 0.5m\n",
            "epoch 1 DONE | 1.8m | VAL loss 1.2061 | VAL macro-F1@0.2 0.0865\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 1.2061275756835939\n",
            "epoch 2 step 50/72 loss 1.1842 sec/step 1.248 ETA 0.5m\n",
            "epoch 2 DONE | 1.8m | VAL loss 1.0553 | VAL macro-F1@0.2 0.0880\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 1.0553174926757813\n",
            "epoch 3 step 50/72 loss 1.0883 sec/step 1.242 ETA 0.5m\n",
            "epoch 3 DONE | 1.8m | VAL loss 1.0404 | VAL macro-F1@0.2 0.0897\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 1.0404491088867187\n",
            "epoch 4 step 50/72 loss 1.0717 sec/step 1.244 ETA 0.5m\n",
            "epoch 4 DONE | 1.8m | VAL loss 1.0266 | VAL macro-F1@0.2 0.0904\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 1.02661611328125\n",
            "epoch 5 step 50/72 loss 1.0508 sec/step 1.254 ETA 0.5m\n",
            "epoch 5 DONE | 1.8m | VAL loss 0.9990 | VAL macro-F1@0.2 0.0941\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.9989518188476563\n",
            "epoch 6 step 50/72 loss 1.0284 sec/step 1.245 ETA 0.5m\n",
            "epoch 6 DONE | 1.8m | VAL loss 0.9756 | VAL macro-F1@0.2 0.1000\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.9755595764160157\n",
            "epoch 7 step 50/72 loss 0.9932 sec/step 1.249 ETA 0.5m\n",
            "epoch 7 DONE | 1.8m | VAL loss 0.9456 | VAL macro-F1@0.2 0.1051\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.9456010864257812\n",
            "epoch 8 step 50/72 loss 0.9709 sec/step 1.248 ETA 0.5m\n",
            "epoch 8 DONE | 1.8m | VAL loss 0.9202 | VAL macro-F1@0.2 0.1208\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.920214697265625\n",
            "epoch 9 step 50/72 loss 0.9593 sec/step 1.255 ETA 0.5m\n",
            "epoch 9 DONE | 1.8m | VAL loss 0.9151 | VAL macro-F1@0.2 0.1202\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.9150665222167969\n",
            "epoch 10 step 50/72 loss 0.9439 sec/step 1.240 ETA 0.5m\n",
            "epoch 10 DONE | 1.8m | VAL loss 0.8951 | VAL macro-F1@0.2 0.1173\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.8950914489746093\n",
            "epoch 11 step 50/72 loss 0.9290 sec/step 1.236 ETA 0.5m\n",
            "epoch 11 DONE | 1.8m | VAL loss 0.9079 | VAL macro-F1@0.2 0.1291\n",
            "epoch 12 step 50/72 loss 0.9261 sec/step 1.247 ETA 0.5m\n",
            "epoch 12 DONE | 1.8m | VAL loss 0.8783 | VAL macro-F1@0.2 0.1258\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.8783094543457032\n",
            "epoch 13 step 50/72 loss 0.9152 sec/step 1.251 ETA 0.5m\n",
            "epoch 13 DONE | 1.8m | VAL loss 0.8777 | VAL macro-F1@0.2 0.1339\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.8777238891601562\n",
            "epoch 14 step 50/72 loss 0.9169 sec/step 1.240 ETA 0.5m\n",
            "epoch 14 DONE | 1.8m | VAL loss 0.8696 | VAL macro-F1@0.2 0.1300\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.8696026000976562\n",
            "epoch 15 step 50/72 loss 0.9133 sec/step 1.253 ETA 0.5m\n",
            "epoch 15 DONE | 1.8m | VAL loss 0.8631 | VAL macro-F1@0.2 0.1306\n",
            "Saved best: /content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth val_loss: 0.8631069641113281\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def eval_macro_f1_thresh(model, loader, thr=0.2):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for wav, y, m, src in loader:\n",
        "            wav = wav.to(DEVICE, non_blocking=True)\n",
        "            probs = torch.sigmoid(model(wav_to_logmel(wav))).cpu().numpy()\n",
        "            ys.append(y.numpy())\n",
        "            ps.append(probs)\n",
        "    y_true = np.concatenate(ys, axis=0).astype(int)\n",
        "    y_prob = np.concatenate(ps, axis=0)\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "EPOCHS = 15\n",
        "LR = 1e-3\n",
        "CLIP_NORM = 1.0\n",
        "LOG_EVERY = 50\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "BEST_PATH = Path(\"/content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth\")\n",
        "BEST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "patience = 3\n",
        "bad = 0\n",
        "steps_per_epoch = len(train_loader)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    running = 0.0\n",
        "\n",
        "    for step, (wav, y, m, src) in enumerate(train_loader, start=1):\n",
        "        wav = wav.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        m = m.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\"):\n",
        "            logmel = wav_to_logmel(wav)\n",
        "            logits = model(logmel)\n",
        "            loss = masked_bce_loss(logits, y, m)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running += loss.item()\n",
        "        if step % LOG_EVERY == 0:\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed = time.time() - t0\n",
        "            sec_per_step = elapsed / step\n",
        "            eta = (steps_per_epoch - step) * sec_per_step\n",
        "            print(f\"epoch {epoch} step {step}/{steps_per_epoch} loss {running/LOG_EVERY:.4f} \"\n",
        "                  f\"sec/step {sec_per_step:.3f} ETA {eta/60:.1f}m\")\n",
        "            running = 0.0\n",
        "\n",
        "    val_loss = eval_val_loss(model, valid_loader)\n",
        "    val_f1_02 = eval_macro_f1_thresh(model, valid_loader, thr=0.2)\n",
        "    print(f\"epoch {epoch} DONE | {((time.time()-t0)/60):.1f}m | VAL loss {val_loss:.4f} | VAL macro-F1@0.2 {val_f1_02:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        bad = 0\n",
        "        torch.save({\"model_state_dict\": model.state_dict(), \"val_loss\": best_loss}, BEST_PATH)\n",
        "        print(\"Saved best:\", BEST_PATH, \"val_loss:\", best_loss)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(f\"Early stopping. Best val_loss: {best_loss:.4f}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yj2HSuijtsv",
        "outputId": "86a2bcde-9c77-4ced-9042-e0a88ba697b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST loss: 0.8252420532226562\n",
            "TEST macro-F1@0.2: 0.12857139411928345\n",
            "TEST macro-F1@0.5: 0.18686141860443403\n"
          ]
        }
      ],
      "source": [
        "BEST_PATH = Path(\"/content/drive/MyDrive/model_weights/finetuning/cbam_openmic_plus_synth_best.pth\")\n",
        "ckpt = torch.load(BEST_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "test_loss = eval_val_loss(model, test_loader)\n",
        "test_f1_02 = eval_macro_f1_thresh(model, test_loader, thr=0.2)\n",
        "test_f1_05 = eval_macro_f1_thresh(model, test_loader, thr=0.5)\n",
        "\n",
        "print(\"TEST loss:\", test_loss)\n",
        "print(\"TEST macro-F1@0.2:\", test_f1_02)\n",
        "print(\"TEST macro-F1@0.5:\", test_f1_05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA9hNf6J27fL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
