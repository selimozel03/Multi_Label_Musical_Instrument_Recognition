{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ev-3BlvOjMNJ"
      },
      "outputs": [],
      "source": [
        "# PRE-TRAINING ON CBAM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QMj_DRcjhYV",
        "outputId": "497d7a90-2c2a-4e38-f3bd-b4f4bf36bee5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/IRMAS-TrainingData\" /content/"
      ],
      "metadata": {
        "id": "UBBf9WBUjnpk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/IRMAS-TrainingData\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD47U17pjqM5",
        "outputId": "432c4868-a47d-4070-f1d7-494c32ddfa46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cel  cla  flu  gac  gel  org  pia  README.txt  sax  tru  vio  voi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4KrbPWm1jyhP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"/content/IRMAS-TrainingData\"\n",
        "SR = 16000\n",
        "N_MELS = 128\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 320\n",
        "TARGET_FRAMES = 151\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-4\n",
        "EPOCHS = 15\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "225h6o43nVot",
        "outputId": "89c33bff-2de8-4bc3-9f29-8fb900deb31f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping"
      ],
      "metadata": {
        "id": "XQt9FjgJnaUd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instrument_to_id = {\n",
        "    \"cel\": 0, \"cla\": 1, \"flu\": 2, \"gac\": 3, \"gel\": 4,\n",
        "    \"pia\": 5, \"sax\": 6, \"tru\": 7, \"vio\": 8\n",
        "}\n",
        "NUM_CLASSES = len(instrument_to_id)\n"
      ],
      "metadata": {
        "id": "fu0FhYxYnedS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IRMASDataset"
      ],
      "metadata": {
        "id": "mv2QW5uTnfwH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IRMASDataset(Dataset):\n",
        "    def __init__(self, root_dir, instrument_to_id):\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "        for inst, lab in instrument_to_id.items():\n",
        "            inst_dir = os.path.join(root_dir, inst)\n",
        "            if not os.path.isdir(inst_dir):\n",
        "                raise FileNotFoundError(f\"Missing folder: {inst_dir}\")\n",
        "            for f in os.listdir(inst_dir):\n",
        "                if f.endswith(\".wav\"):\n",
        "                    self.files.append(os.path.join(inst_dir, f))\n",
        "                    self.labels.append(lab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def _fix_frames(self, mel, target_frames):\n",
        "        # mel: [F, T]\n",
        "        T = mel.shape[1]\n",
        "        if T == target_frames:\n",
        "            return mel\n",
        "        if T > target_frames:\n",
        "            return mel[:, :target_frames]\n",
        "        pad = target_frames - T\n",
        "        # pad sağa (min değerle)\n",
        "        return F.pad(mel, (0, pad), value=float(mel.min()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y, _ = librosa.load(self.files[idx], sr=SR, mono=True)\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y, sr=SR, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH\n",
        "        )\n",
        "        mel = librosa.power_to_db(mel)\n",
        "        mel = torch.tensor(mel, dtype=torch.float32)       # [F,T]\n",
        "        mel = self._fix_frames(mel, TARGET_FRAMES)         # [F,Tfixed]\n",
        "        mel = mel.unsqueeze(0)                             # [1,F,T]\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return mel, label\n"
      ],
      "metadata": {
        "id": "fyKZBqgWniCY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader + sanity check"
      ],
      "metadata": {
        "id": "loKNamJUnkg3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = IRMASDataset(ROOT_DIR, instrument_to_id)\n",
        "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "xb, yb = next(iter(dl))\n",
        "print(\"Mel:\", xb.shape, \"Labels:\", yb.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhPJHvuYnogN",
        "outputId": "1c60810a-bdcf-4fbd-e7ce-79a4b7c537a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mel: torch.Size([32, 1, 128, 151]) Labels: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CBAM model"
      ],
      "metadata": {
        "id": "3I7uz2yknp2V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_ch, r=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_ch, max(1, in_ch//r)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(max(1, in_ch//r), in_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,C,H,W]\n",
        "        avg = torch.mean(x, dim=(2,3))\n",
        "        mx  = torch.amax(x, dim=(2,3))\n",
        "        att = torch.sigmoid(self.mlp(avg) + self.mlp(mx)).unsqueeze(-1).unsqueeze(-1)\n",
        "        return x * att\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, k=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=k, padding=k//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg = torch.mean(x, dim=1, keepdim=True)\n",
        "        mx  = torch.amax(x, dim=1, keepdim=True)\n",
        "        att = torch.sigmoid(self.conv(torch.cat([avg, mx], dim=1)))\n",
        "        return x * att\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(ch)\n",
        "        self.sa = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sa(self.ca(x))\n",
        "\n",
        "class CBAM_CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            CBAM(32),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            CBAM(64),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x).flatten(1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "E4AzSxNHnsjb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop + kaydetme"
      ],
      "metadata": {
        "id": "eQRKZPDjnu1v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CBAM_CNN(NUM_CLASSES).to(DEVICE)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss, correct, n = 0.0, 0, 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        n += yb.size(0)\n",
        "\n",
        "    print(f\"Epoch {ep:02d} | loss {total_loss/len(dl):.4f} | acc {correct/n:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/cbam_irmas_features.pth\"\n",
        "torch.save(model.features.state_dict(), save_path)\n",
        "print(\"Saved CBAM features to:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9NZLruRnxbZ",
        "outputId": "c730b336-83b1-451f-abf7-32fbbac87eda"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | loss 2.1438 | acc 0.202 | 130.4s\n",
            "Epoch 02 | loss 2.0132 | acc 0.264 | 132.4s\n",
            "Epoch 03 | loss 1.9320 | acc 0.296 | 133.0s\n",
            "Epoch 04 | loss 1.8600 | acc 0.329 | 130.7s\n",
            "Epoch 05 | loss 1.7984 | acc 0.350 | 134.3s\n",
            "Epoch 06 | loss 1.7670 | acc 0.354 | 130.6s\n",
            "Epoch 07 | loss 1.7159 | acc 0.376 | 132.0s\n",
            "Epoch 08 | loss 1.6813 | acc 0.387 | 131.1s\n",
            "Epoch 09 | loss 1.6435 | acc 0.404 | 131.7s\n",
            "Epoch 10 | loss 1.6238 | acc 0.406 | 130.6s\n",
            "Epoch 11 | loss 1.6061 | acc 0.417 | 134.7s\n",
            "Epoch 12 | loss 1.5676 | acc 0.434 | 130.1s\n",
            "Epoch 13 | loss 1.5405 | acc 0.443 | 134.1s\n",
            "Epoch 14 | loss 1.5249 | acc 0.446 | 133.1s\n",
            "Epoch 15 | loss 1.5106 | acc 0.453 | 129.8s\n",
            "Saved CBAM features to: /content/drive/MyDrive/cbam_irmas_features.pth\n"
          ]
        }
      ]
    }
  ]
}